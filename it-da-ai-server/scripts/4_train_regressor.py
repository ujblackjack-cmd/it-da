"""
Step 4: ê°ì„± í”¼ì²˜ë¥¼ í¬í•¨í•œ LightGBM Regressor ì¬í•™ìŠµ
"""

import pandas as pd
import numpy as np
import pickle
import warnings
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import lightgbm as lgb
import sys
import os

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

# ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from app.core.feature_builder import FeatureBuilder

warnings.filterwarnings('ignore')
os.environ['LIGHTGBM_VERBOSITY'] = '-1'


def load_training_data():
    """í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
    print("\n[1/5] í•™ìŠµ ë°ì´í„° ë¡œë”©...")
    df = pd.read_csv('data/training_data.csv')

    meeting_sentiment = pd.read_csv('data/meeting_sentiment.csv')

    print(f"[OK] {len(df):,}ê°œ ìƒ˜í”Œ ë¡œë“œ")
    print(f"[OK] {len(meeting_sentiment):,}ê°œ ëª¨ì„ ê°ì„± ë°ì´í„° ë¡œë“œ")

    return df, meeting_sentiment


def prepare_features(df: pd.DataFrame, meeting_sentiment: pd.DataFrame, feature_builder: FeatureBuilder):
    """FeatureBuilderë¡œ íŠ¹ì§• ë²¡í„° ìƒì„±"""

    print("\n[2/5] Feature ìƒì„± ì¤‘...")

    df = df.merge(meeting_sentiment[['meeting_id', 'avg_sentiment_score',
                                      'positive_review_ratio', 'negative_review_ratio',
                                      'review_sentiment_variance']],
                  on='meeting_id', how='left', suffixes=('', '_agg'))

    X_list = []
    y_list = []

    for idx, row in df.iterrows():
        if idx % 1000 == 0:
            print(f"  ì§„í–‰: {idx:,} / {len(df):,}")

        user = {
            "lat": row['user_lat'],
            "lng": row['user_lng'],
            "interests": row['user_interests'],
            "time_preference": row['time_preference'],
            "user_location_pref": row['user_location_pref'],
            "budget_type": row['budget_type'],
            "user_avg_rating": row['user_avg_rating'],
            "user_meeting_count": row['user_meeting_count'],
            "user_rating_std": row['user_rating_std'],
            "gender": row['user_gender'],
            "mbti": row['user_mbti'],
        }

        meeting = {
            "lat": row['meeting_lat'],
            "lng": row['meeting_lng'],
            "category": row['category'],
            "subcategory": row.get('subcategory', ''),
            "time_slot": row['time_slot'],
            "meeting_location_type": row['meeting_location_type'],
            "vibe": row['vibe'],
            "max_participants": row['max_participants'],
            "expected_cost": row['expected_cost'],
            "meeting_avg_rating": row['meeting_avg_rating'],
            "meeting_rating_count": row['meeting_rating_count'],
            "meeting_participant_count": row['meeting_participant_count'],

            "sentiment": {
                "avg_sentiment_score": row.get('avg_sentiment_score_agg', row.get('avg_sentiment_score', 0.5)),
                "positive_review_ratio": row.get('positive_review_ratio_agg', row.get('positive_review_ratio', 0.5)),
                "negative_review_ratio": row.get('negative_review_ratio_agg', row.get('negative_review_ratio', 0.5)),
                "review_sentiment_variance": row.get('review_sentiment_variance_agg', row.get('review_sentiment_variance', 0.0)),
            }
        }

        try:
            _, X = feature_builder.build(user, meeting)
            X_list.append(X[0])
            y_list.append(row['rating'])
        except Exception as e:
            continue

    X = np.array(X_list)
    y = np.array(y_list)

    print(f"[OK] Feature ìƒì„± ì™„ë£Œ: {X.shape}")

    return X, y


def train_regressor():
    """LightGBM Regressor í•™ìŠµ"""

    print("=" * 70)
    print("[Step 4] LightGBM Regressor ì¬í•™ìŠµ")
    print("=" * 70)

    df, meeting_sentiment = load_training_data()

    feature_builder = FeatureBuilder()
    print(f"[OK] FeatureBuilder ì¤€ë¹„ (ì´ {feature_builder.n_features}ê°œ features)")

    X, y = prepare_features(df, meeting_sentiment, feature_builder)

    print("\n[3/5] Train/Test Split...")

    # âœ… ìµœì†Œ ë°ì´í„° ì²´í¬
    if len(X) < 10:
        print(f"[WARNING] ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ ({len(X)}ê°œ)")
        print("[INFO] ìµœì†Œ 10ê°œ ì´ìƒì˜ ë¦¬ë·°ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        print("[INFO] í•™ìŠµì„ ê±´ë„ˆë›°ê³  ê¸°ì¡´ ëª¨ë¸ì„ ìœ ì§€í•©ë‹ˆë‹¤")
        print("\n" + "=" * 70)
        print("[ì™„ë£Œ] ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ í•™ìŠµ ê±´ë„ˆëœ€")
        print("=" * 70)
        print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
        print("  1. Spring Bootì—ì„œ ë” ë§ì€ ë¦¬ë·° ì‘ì„±")
        print("  2. ìµœì†Œ 10ê°œ ì´ìƒì˜ ë¦¬ë·° í•„ìš” (ê¶Œì¥: 100ê°œ ì´ìƒ)")
        print("  3. ë‹¤ì–‘í•œ í‰ì (1~5)ê³¼ ëª¨ì„ ì¹´í…Œê³ ë¦¬ í•„ìš”")
        return

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, shuffle=True
    )
    print(f"[OK] Train: {len(X_train):,}, Test: {len(X_test):,}")

    print("\n[4/5] LightGBM í•™ìŠµ ì¤‘...")

    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'max_depth': 6,
        'min_child_samples': 20,
    }

    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

    model = lgb.train(
        params,
        train_data,
        num_boost_round=500,
        valid_sets=[valid_data],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50),
            lgb.log_evaluation(period=50)
        ]
    )

    print("[OK] í•™ìŠµ ì™„ë£Œ")

    print("\n[5/5] ëª¨ë¸ í‰ê°€...")
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    print("\n[í†µê³„] Train ì„±ëŠ¥:")
    print(f"  RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.4f}")
    print(f"  MAE:  {mean_absolute_error(y_train, y_pred_train):.4f}")
    print(f"  R2:   {r2_score(y_train, y_pred_train):.4f}")

    print("\n[í†µê³„] Test ì„±ëŠ¥:")
    print(f"  RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}")
    print(f"  MAE:  {mean_absolute_error(y_test, y_pred_test):.4f}")
    print(f"  R2:   {r2_score(y_test, y_pred_test):.4f}")

    print("\n[í†µê³„] Top 15 Feature Importance:")
    importance = model.feature_importance(importance_type='gain')
    feature_names = feature_builder.get_feature_names()

    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importance
    }).sort_values('importance', ascending=False).head(15)

    for idx, row in importance_df.iterrows():
        print(f"  {row['feature']:<30} {row['importance']:>10.1f}")

    print("\n[INFO] ëª¨ë¸ ì €ì¥ ì¤‘...")
    os.makedirs('models', exist_ok=True)

    payload = {
        "model": model,
        "regressor": model,
        "feature_names": feature_names,
        "schema_version": "v2_with_sentiment",
        "scaler": None
    }

    model_path = 'models/lightgbm_regressor.pkl'
    with open(model_path, 'wb') as f:
        pickle.dump(payload, f)

    print(f"[OK] ëª¨ë¸ ì €ì¥: {model_path}")

    print("\n" + "=" * 70)
    print("[ì™„ë£Œ] ì¬í•™ìŠµ ì™„ë£Œ!")
    print("=" * 70)

    return model


if __name__ == "__main__":
    train_regressor()